# Ophtimus: Ophthalmology-specific LLM
<p align="center">
<br>
    <picture>
        <img alt="Ophtimus Logo" src="Images/Ophtimus_logo.png" width="50%"">
    </picture>
</br>

<p align="center"> ğŸ¤— <a href="https://huggingface.co/collections/BaekSeungJu/ophtimus-series-67d859fedb756527d680ce42">Models and Datasets</a> &nbsp | &nbsp ğŸ“• <a href="https://drive.google.com/file/d/11dpyQioF3eshol3asbFzsNAJ4cEqwm49/view?usp=sharing">AAAI 2025 workshop Paper</a>


## Introduction
ì•ˆê³¼ ë¶„ì•¼ì— íŠ¹í™”ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì¸ Ophtimusë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. OphtimusëŠ” ì˜ë£Œ ì¸ê³µì§€ëŠ¥ì˜ ì‹¤ìš©ì„±ê³¼ ì ‘ê·¼ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ê°œë°œëœ 80ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ì „ë¬¸ ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë²”ìš© LLMì´ íŠ¹ì • ì˜ë£Œ ë¶„ì•¼ì—ì„œ í•œê³„ë¥¼ ë³´ì´ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì, OphtimusëŠ” ì•ˆê³¼ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµê³¼ ë¯¸ì„¸ ì¡°ì •ì„ ê±°ì³ ê³ ë„ì˜ ë„ë©”ì¸ ì§€ì‹ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. 
OphtimusëŠ” ì•ˆê³¼ ì „ë¬¸ì„±ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ ë„ë©”ì¸ íŠ¹í™” ë°ì´í„° í•™ìŠµì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ë£Œ ë…¼ë¬¸, êµê³¼ì„œ, ì—°êµ¬ ë³´ê³ ì„œ ë“±ì˜ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ì„ ë³„í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ì„±ì„ ë†’ì˜€ìœ¼ë©°, ì´ë¥¼ í†µí•´ ì¼ë°˜ì ì¸ ì˜ë£Œ AIë³´ë‹¤ ë” ì •êµí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì•ˆê³¼ ì§€ì‹ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, í•™ìŠµ ê³¼ì •ì—ì„œ ë°ì´í„° í•„í„°ë§, ìš”ì•½ ë° ì „ì²˜ë¦¬ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ë°°ì œí•˜ê³  í•µì‹¬ ë‚´ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. 
ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” Metaì˜ LLaMA ì‹œë¦¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê³ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„ ê²½ëŸ‰í™”ëœ êµ¬ì¡°ë¥¼ ì±„íƒí•˜ì—¬ ì˜ë£Œ í™˜ê²½ì—ì„œ ì‹¤ì œë¡œ í™œìš© ê°€ëŠ¥í•œ íš¨ìœ¨ì ì¸ AI ì‹œìŠ¤í…œì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ OphtimusëŠ” ë‹¨ìˆœí•œ ì—°êµ¬ìš© ëª¨ë¸ì„ ë„˜ì–´ ì„ìƒ ì§€ì›, ì˜ë£Œ êµìœ¡, í™˜ì ìƒë‹´ ë³´ì¡° ë“±ì˜ ì‹¤ì§ˆì ì¸ ì‘ìš© ê°€ëŠ¥ì„±ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
Ophtimusì˜ ì¤‘ìš”í•œ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ê°œë°©ì„±ê³¼ í™•ì¥ì„±ì…ë‹ˆë‹¤. ì˜ë£Œ AIì˜ ë°œì „ì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì–´ ëˆ„êµ¬ë‚˜ í™œìš©í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—°êµ¬ì, ì˜ë£Œì§„, ê°œë°œìë“¤ì´ ë³´ë‹¤ ì‰½ê²Œ ì•ˆê³¼ ì „ë¬¸ LLMì„ í™œìš©í•˜ê³ , ë§ì¶¤í˜• ì˜ë£Œ AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, Ophtimusì˜ ê°œë°œ ê³¼ì •ê³¼ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ë„ ê³µê°œë˜ì–´ ìˆì–´, ë‹¤ë¥¸ ì˜ë£Œ ë¶„ì•¼ì—ì„œë„ ìœ ì‚¬í•œ ë„ë©”ì¸ íŠ¹í™” LLMì„ ê°œë°œí•˜ëŠ” ë° ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

<p align="left">
 <img src="./Images/Ophtimus_Dev_Architecture2.png" width="80%">
</p>

## Model Details

> [!Note]
> Tableì˜ pre-training, fine-tuningì€ ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì‹¤í–‰í•œ í•™ìŠµì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  
> Base modelë“¤ì€ ì´ë¯¸ ì´ì „ì— pre-training/fine-tuningì„ ê±°ì¹œ ëª¨ë¸ë“¤ë¡œ ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” transfer learning í•˜ì˜€ìŠµë‹ˆë‹¤.

| ëª¨ë¸ëª… | Base model | íŒŒë¼ë¯¸í„° | Pre-training | Fine-tuning |
|------|-------------|------------|-------------|------------|
| Ophtimus-Base [[Link](https://huggingface.co/BaekSeungJu/Ophtimus-8B-Base)] | Llama-3.1-8B | 8B | âœ… | âŒ |
| Ophtimus-Llama-1B [[Link](https://huggingface.co/BaekSeungJu/Ophtimus-1B-Instruct)] | Llama-3.2-1B-Instruct | 1B | âŒ | âœ… |
| Ophtimus-Llama-3B [[Link](https://huggingface.co/BaekSeungJu/Ophtimus-3B-Instruct)]| Llama-3.2-3B-Instruct | 3B | âŒ | âœ… |
| Ophtimus-Llama-8B [[Link](https://huggingface.co/BaekSeungJu/Ophtimus-8B-Instruct)] | Llama-3.1-8B-Instruct | 8B | âŒ | âœ… |
| Ophtimus-Instruct-8B [[Link](https://huggingface.co/your-link-here)] | Ophtimus-Base | 8B |âœ… | âœ… |

## Performance

> [!Note]
> Multi-Choice QA : Ophtimus-Eval, MedMCQA, PubMedQA | Essay QA : MedQuAD, Medical Flashcards, Medical Wikidoc
> Ophtimus-Evalì€ ìì²´ì ìœ¼ë¡œ ì˜ë£Œ í”Œë ›í¼ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì´ê³ , ë‚˜ë¨¸ì§€ëŠ” ì˜ë£Œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì—ì„œ ì•ˆê³¼í•™ ë„ë©”ì¸ QAë§Œì„ ì¶”ì¶œí•˜ì—¬ í‰ê°€í•˜ì˜€ìŠµë‹ˆë‹¤

<table>
    <thead>
        <tr>
            <th rowspan="2">Model</th>
            <th colspan="3">Multi-Choice Question</th>
            <th colspan="4">Essay Question</th>
        </tr>
        <tr>
            <th>Ophtimus Eval</th>
            <th>MedMCQA (Ophth)</th>
            <th>PubmedQA (Ophth)</th>
            <th>RougeL</th>
            <th>BLEU</th>
            <th>METEOR</th>
            <th>SemScore</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="highlight">OpenAI GPT-4o</td>
            <td class="highlight">71.95%</td>
            <td class="highlight">81.95%</td>
            <td class="highlight">89.90%</td>
            <td>0.193</td>
            <td>0.082</td>
            <td class="highlight">0.341</td>
            <td class="highlight">0.761</td>
        </tr>
        <tr>
            <td>Llama-3-8B-Instrct</td>
            <td>48.60%</td>
            <td>74.02%</td>
            <td>63.97%</td>
            <td>0.193</td>
            <td>0.064</td>
            <td>0.244</td>
            <td>0.684</td>
        </tr>
        <tr>
            <td>Llama-3.1-8B-Instrct</td>
            <td>39.78%</td>
            <td>57.96%</td>
            <td>83.84%</td>
            <td>0.177</td>
            <td>0.054</td>
            <td>0.215</td>
            <td>0.641</td>
        </tr>
        <tr>
            <td>Eye-Llama</td>
            <td>32.56%</td>
            <td>59.43%</td>
            <td>66.11%</td>
            <td>0.183</td>
            <td>0.062</td>
            <td>0.211</td>
            <td>0.686</td>
        </tr>
        <tr>
            <td>PMC-Llama-13B</td>
            <td>48.28%</td>
            <td>63.45%</td>
            <td>72.48%</td>
            <td>0.223</td>
            <td>0.082</td>
            <td>0.288</td>
            <td>0.714</td>
        </tr>
        <tr>
            <td>Ophtimus-Llama-1B</td>
            <td>41.45%</td>
            <td>45.74%</td>
            <td>61.95%</td>
            <td>0.219</td>
            <td>0.076</td>
            <td>0.217</td>
            <td>0.711</td>
        </tr>
        <tr>
            <td>Ophtimus-Llama-3B</td>
            <td>52.70%</td>
            <td>62.10%</td>
            <td>69.36%</td>
            <td>0.224</td>
            <td>0.077</td>
            <td>0.225</td>
            <td>0.726</td>
        </tr>
        <tr>
            <td>Ophtimus-Llama-8B</td>
            <td>60.78%</td>
            <td>68.25%</td>
            <td>69.70%</td>
            <td class="highlight">0.226</td>
            <td class="highlight">0.083</td>
            <td>0.230</td>
            <td>0.733</td>
        </tr>
        <tr>
            <td>Ophtimus-Instruct-8B</td>
            <td>63.85%</td>
            <td>71.51%</td>
            <td>72.73%</td>
            <td>0.222</td>
            <td>0.079</td>
            <td>0.224</td>
            <td class="highlight">0.735</td>
        </tr>
    </tbody>
</table>

</body>
</html>

## Quickstart

### Install Dependencies

```bash
cd Ophtimus-Ophthalmology-LLM
pip install -r requirements.txt
```

### Ophtimus Inference

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# model_name example : BaekSeungJu/Ophtimus-Instruct-8B or Ophtimus-Llama-1B or Ophtimus-Llama-3B or Ophtimus-Llama-8B
model_name = "BaekSeungJu/Ophtimus-Instruct-8B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
).to("cuda")

tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")
tokenizer.pad_token = tokenizer.eos_token

system_instruction = (
    "You are an expert ophthalmologist. Please provide accurate and "
    "medically sound answers to the user's ophthalmology-related question."
)

# ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŠµë‹ˆë‹¤.
questions = [
    "Please describe the symptoms and treatment of epiretinal membrane.",
    "What's good for eyes?"
]

prompts = []
for question in questions:
    row_json = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": question}
    ]
    prompt = tokenizer.apply_chat_template(row_json, add_generation_prompt=True, tokenize=False)
    prompts.append(prompt)

input_ids = tokenizer(
    prompts,
    padding=True,
    return_tensors="pt",
)["input_ids"].to("cuda")

model.eval()
with torch.no_grad():
    outputs = model.generate(
        input_ids,
        max_new_tokens=1024,
        do_sample=False,
    )

decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)
for i, text in enumerate(decoded):
    print(f"------------------------\nAnswer for question {i+1}:\n{text}")
```

## Pre-Training

```bash
python Pre-Trainin.py --num_train_epochs 3 --per_device_train_batch_size 4
```
