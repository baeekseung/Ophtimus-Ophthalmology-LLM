{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a634bd26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T09:57:38.964712Z",
     "start_time": "2024-09-28T09:57:38.387420Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HF_TOKEN_read\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d255194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8caa63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=123 environment variable detected; loading libbitsandbytes_cuda123.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a59261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1984893e14b54f7d87fed29f5c502206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2523ddec173c40b89eb0bece7f7cf38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcd2eec780448d9aaa5ea7e836e0450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574d237d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a276a3061f7d41349a75ac3bfc1e7eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5e7202b49541508387b7f4105946d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fabfa2f02b420ea4fcac1db1b256b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|finetune_right_pad_id|>\"})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # 128004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4778813d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de66ec32f7ef47cdb14165c1547de1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/317 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ae96d3cb034409980fa4235cbdfa75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17b2f32e4ed4e7faa9794d8828ea361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/49300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b200eaded6b496785835a8d445ef46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/49300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 48807\n",
      "Test dataset size: 493\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "EQA_dataset = load_dataset(\"BaekSeungJu/Ophthalmology-EQA-v3\", split=\"train\")\n",
    "EQA_dataset = EQA_dataset.shuffle(seed=42)\n",
    "\n",
    "def EQA_format_chat_template(row):\n",
    "    system_instruction = \"You are an expert ophthalmologist. Please provide accurate and medically answers to the user's ophthalmology-related question.\"\n",
    "    \n",
    "    row_json = [{\"role\": \"system\", \"content\": system_instruction },\n",
    "               {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"answer\"]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "mapped_EQA_dataset = EQA_dataset.map(\n",
    "    EQA_format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "\n",
    "split_EQA_dataset = mapped_EQA_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_EQA_dataset = split_EQA_dataset['train']\n",
    "test_EQA_dataset = split_EQA_dataset['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_EQA_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_EQA_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b67ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089d5e4ad6fd489fa47092e5ac73fd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/542 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab7333fe827422bba48853218a8512d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a92e7132c8b409180a0b6f496869fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0761a42ab347768ca0712c1924a67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/51745 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 51227\n",
      "Test dataset size: 518\n"
     ]
    }
   ],
   "source": [
    "MCQA_dataset = load_dataset(\"BaekSeungJu/Ophthalmology-MCQA-v3\", split=\"train\")\n",
    "MCQA_dataset = MCQA_dataset.shuffle(seed=42)\n",
    "\n",
    "def MCQA_format_chat_template(row):\n",
    "    # System instruction\n",
    "    system_instruction = (\n",
    "        \"You are an expert ophthalmologist. Please provide accurate and \"\n",
    "        \"medically sound answers to the user's ophthalmology-related question.\"\n",
    "    )\n",
    "    \n",
    "    # 3. Define helper functions\n",
    "    def MCQA_Alphabet_Selection_func(answer, option_a, option_b, option_c, option_d, option_e):\n",
    "        if answer == option_a:\n",
    "            return \"A\"\n",
    "        elif answer == option_b:\n",
    "            return \"B\"\n",
    "        elif answer == option_c:\n",
    "            return \"C\"\n",
    "        elif answer == option_d:\n",
    "            return \"D\"\n",
    "        elif answer == option_e:\n",
    "            return \"E\"\n",
    "        return \"\"  # fallback if none match\n",
    "    \n",
    "    def MCQA_Instruction_formatting_func(question, option_a, option_b, option_c, option_d, option_e):\n",
    "        \"\"\"Format the question and the available answer choices.\"\"\"\n",
    "        # Check how many valid options there are to display\n",
    "        if option_c == \"\":\n",
    "            # Only A and B\n",
    "            return (f\"Question:\\n{question}\\n\\nOptions:\\n\"\n",
    "                    f\"A) {option_a}\\nB) {option_b}\")\n",
    "        elif option_e == \"\":\n",
    "            # Options A, B, C, D\n",
    "            return (f\"Question:\\n{question}\\n\\nOptions:\\n\"\n",
    "                    f\"A) {option_a}\\nB) {option_b}\\nC) {option_c}\\nD) {option_d}\")\n",
    "        else:\n",
    "            # Options A, B, C, D, E\n",
    "            return (f\"Question:\\n{question}\\n\\nOptions:\\n\"\n",
    "                    f\"A) {option_a}\\nB) {option_b}\\nC) {option_c}\\nD) {option_d}\\nE) {option_e}\")\n",
    "    \n",
    "    def MCQA_Response_formatting_func(option_a, option_b, option_c, option_d, option_e, explanation, answer):\n",
    "        \"\"\"Format the explanation and the final answer.\"\"\"\n",
    "        letter = MCQA_Alphabet_Selection_func(answer, option_a, option_b, option_c, option_d, option_e)\n",
    "        return f\"Explanation:\\n{explanation}\\n\\nAnswer:\\n{letter}) {answer}\"\n",
    "    \n",
    "    # 4. Build the chat template JSON\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": MCQA_Instruction_formatting_func(\n",
    "                row[\"question\"],\n",
    "                row[\"option_a\"],\n",
    "                row[\"option_b\"],\n",
    "                row[\"option_c\"],\n",
    "                row[\"option_d\"],\n",
    "                row[\"option_e\"]\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": MCQA_Response_formatting_func(\n",
    "                row[\"option_a\"],\n",
    "                row[\"option_b\"],\n",
    "                row[\"option_c\"],\n",
    "                row[\"option_d\"],\n",
    "                row[\"option_e\"],\n",
    "                row[\"explanation\"],\n",
    "                row[\"answer\"]\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 5. Convert or tokenize the row_json however you like\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    \n",
    "    return row\n",
    "\n",
    "mapped_MCQA_dataset = MCQA_dataset.map(\n",
    "    MCQA_format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "split_MCQA_dataset = mapped_MCQA_dataset.train_test_split(test_size=0.01, seed=42)\n",
    "train_MCQA_dataset = split_MCQA_dataset['train']\n",
    "test_MCQA_dataset = split_MCQA_dataset['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_MCQA_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_MCQA_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9eec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 101371\n",
      "Test dataset size: 1025\n"
     ]
    }
   ],
   "source": [
    "train_dataset = concatenate_datasets([train_EQA_dataset, train_MCQA_dataset])\n",
    "test_dataset = concatenate_datasets([test_EQA_dataset, test_MCQA_dataset])\n",
    "train_dataset=train_dataset.shuffle(seed=42)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38839ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 25 Jan 2025\n",
      "\n",
      "You are an expert ophthalmologist. Please provide accurate and medically answers to the user's ophthalmology-related question.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the effect of damage to the oculomotor parasympathetic fibers on the pupil?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Damage to the oculomotor parasympathetic fibers results in miosis, which is the constriction of the pupil.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adad6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "data_collator_param = {}\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
    "data_collator_param[\"data_collator\"] = collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e289760",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_output_dir = \"../../Ophtimus_LoRA/Ophtimus_8B_Instruct_checkpoint\"\n",
    "\n",
    "import os\n",
    "os.makedirs(local_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02b6da06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3d4c0cdac576aac8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3d4c0cdac576aac8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{local_output_dir}/runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cbbd493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_126/258253107.py:28: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/home/work/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f7a7470cce473bb8974c5e7cd41c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514064365ee4771999f5bad8c95dec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training setup\n",
    "from trl import SFTTrainer,SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "  output_dir=local_output_dir,\n",
    "  report_to = \"tensorboard\",\n",
    "  per_device_train_batch_size = 8,\n",
    "  per_device_eval_batch_size = 8,\n",
    "  gradient_accumulation_steps = 8,\n",
    "  warmup_steps = 10,\n",
    "  num_train_epochs=5,\n",
    "  eval_steps=25,\n",
    "#   save_steps=100,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  save_strategy=\"epoch\",\n",
    "  learning_rate = 2e-4,\n",
    "  logging_steps = 1,\n",
    "  optim = \"adamw_torch\",\n",
    "  weight_decay = 0.01,\n",
    "  max_seq_length = 1024,\n",
    "  lr_scheduler_type = \"constant_with_warmup\",\n",
    "  seed = 42,\n",
    "  gradient_checkpointing = True,\n",
    "  gradient_checkpointing_kwargs={'use_reentrant':True}\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    peft_config = peft_config,\n",
    "    args = training_arguments,\n",
    "    **data_collator_param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c9fb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='7920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  13/7920 00:26 < 5:11:48, 0.42 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460158d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lossPD = pd.DataFrame(trainer.state.log_history)\n",
    "lossPD.to_csv(f\"./{local_output_dir}/loss.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd375d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.3 (NGC 24.04/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
