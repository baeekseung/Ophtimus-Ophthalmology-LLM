{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:32:50.480739Z",
     "start_time": "2024-08-08T09:32:50.474983Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "HF_TOKEN_read = os.getenv(\"HF_TOKEN_read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdf9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token = HF_TOKEN_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217eb03af40b047a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:20.035803Z",
     "start_time": "2024-08-08T09:33:10.169853Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pretraining_dataset = load_dataset(\"BaekSeungJu/Ophthalmology-PubMed-Corpus\")\n",
    "print(pretraining_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5befc5f5ffc66e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:25.569364Z",
     "start_time": "2024-08-08T09:33:25.564328Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pretraining_dataset.select_columns(\n",
    "    [\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762071c28db13410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:27.718696Z",
     "start_time": "2024-08-08T09:33:27.713992Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset[\"train\"][9][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6ef76678595c2",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "\n",
    "In the cells below, you'll carry out the following cleaning steps:\n",
    "1. Filter out samples that are too short\n",
    "2. Remove repetitions within a single text example\n",
    "3. Remove duplicated documents\n",
    "4. Quality filter to remove non-English texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e50634485f4cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:34.878997Z",
     "start_time": "2024-08-08T09:33:34.874013Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af1f678e7e9029",
   "metadata": {},
   "source": [
    "### Remove repeated text within training examples\n",
    "\n",
    "Here you'll remove text repetitions within each example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04ddcbe459e451e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:43.209675Z",
     "start_time": "2024-08-08T09:33:43.205693Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_duplicates(paragraphs):\n",
    "    \"\"\"\n",
    "    Use this function to find the number of repetitions \n",
    "    in the paragraphs.\n",
    "    \"\"\"\n",
    "    unique_x = set()\n",
    "    duplicate_chars = 0\n",
    "    duplicate_elements = 0\n",
    "    for element in paragraphs:\n",
    "        if element in unique_x:\n",
    "            duplicate_chars += len(element)\n",
    "            duplicate_elements += 1\n",
    "        else:\n",
    "            unique_x.add(element)\n",
    "    return duplicate_elements, duplicate_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ca24d6b82339c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:46.693723Z",
     "start_time": "2024-08-08T09:33:46.689326Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def paragraph_repetition_filter(x):\n",
    "    \"\"\"\n",
    "    Returns False iff a page has too many repetitions.\n",
    "    \"\"\"\n",
    "    text = x['text']\n",
    "    paragraphs = re.compile(r\"\\n{2,}\").split(text.strip())                # Split by paragraphs (2 or more newlines)\n",
    "    paragraphs_duplicates, char_duplicates = find_duplicates(paragraphs)  # Find number of duplicates in paragraphs\n",
    "    if paragraphs_duplicates / len(paragraphs) > 0.3:\n",
    "        return False\n",
    "    if char_duplicates / len(text) > 0.2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d79f4e85f3fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:52.632119Z",
     "start_time": "2024-08-08T09:33:52.077153Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.filter(\n",
    "    paragraph_repetition_filter,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36cb7034128760d",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "In this section, you'll remove duplicate examples from the entire dataset (in contrast to the previous step where you were just looking for repeated text in each example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503073c72c85ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:33:59.526157Z",
     "start_time": "2024-08-08T09:33:59.383588Z"
    }
   },
   "outputs": [],
   "source": [
    "def deduplication(ds):\n",
    "    def dedup_func(x):\n",
    "        \"\"\"Use this function to remove duplicate entries\"\"\"\n",
    "        if x['text'] in unique_text:\n",
    "            return False\n",
    "        else:\n",
    "            unique_text.add(x['text'])\n",
    "            return True\n",
    "\n",
    "    unique_text = set()\n",
    "\n",
    "    ds = ds.filter(dedup_func, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = deduplication(dataset)\n",
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed6c46e45b5863",
   "metadata": {},
   "source": [
    "### Quality filter - Language\n",
    "\n",
    "Here you'll remove any text examples that are in a language other than English. The code here uses a language detection model called fastText. You can read about fastText [here](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2887123bdb8170f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:34:13.341988Z",
     "start_time": "2024-08-08T09:34:08.704160Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from fasttext.FastText import _FastText\n",
    "\n",
    "def english_language_filter(ds):\n",
    "    # load language detection model\n",
    "    model = _FastText('./models/L2_language_model.bin')\n",
    "    \n",
    "    def is_english(x):\n",
    "        # Predict language of the text and probability\n",
    "        language, score = model.predict(x['text'].replace(\"\\n\", \"\"))\n",
    "\n",
    "        language = language[0].split(\"__\")[2]\n",
    "        return score > 0.4 and language == \"en\" # change code here if building a model in another language\n",
    "\n",
    "    ds = ds.filter(is_english, load_from_cache_file=False, num_proc=1)\n",
    "    return ds\n",
    "\n",
    "dataset = english_language_filter(dataset)\n",
    "\n",
    "dataset.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8065e244e64da8",
   "metadata": {},
   "source": [
    "## 3. Save the dataset to disk\n",
    "\n",
    "Read more about the parquet data format [here](https://parquet.apache.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72192521b630a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T09:34:48.429145Z",
     "start_time": "2024-08-08T09:34:48.238861Z"
    }
   },
   "outputs": [],
   "source": [
    "directory = \"./Pre-Training-Dataset\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_path = os.path.join(directory, \"Preprocessed_pretrain_Dataset.parquet\")\n",
    "dataset[\"train\"].to_parquet(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
